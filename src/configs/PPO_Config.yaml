---
# run info
run_name: "ppo"
n_episodes: 50
seed: 42
n_workers: 2
max_optimisation_steps: 1000

# Learner Config
learner_config: 
    max_steps: 1000
    max_steps_per_episode: 200
    epochs_per_rollout: 4
    batch_size: 32
    IS: false
    

# Controller Config
controller_config: 
    action_size: 5
    neighbour_depth: 2
    optimiser_config:
      type: "adam"
      learning_rate: 1e-5
    batch_size: 32
    gae_horizon: 16
    n_epochs_update: 3
    gamma: 0.995
    lam: 0.95
    clip_epsilon: 0.2
    value_loss_coefficient: 0.5
    entropy_coefficient: 0.01
    tau: 0.99
    n_features: 12
    actor_config:
      n_heads: 5
      hidden_size: 128
      intent_size: 32
      neighbour_depth: 3
      layer_sizes: 
      - 256
      - 128
    critic_config:
      layer_sizes: 
      - 256
      - 128


# Environment Config
environment_config:
    height: 30
    width: 30
    n_agents: 8
    n_cities: 4
    grid_distribution: false
    max_rails_between_cities: 2
    max_rail_pairs_in_city: 2
    observation_builder_config: 
      type: "tree"
      predictor: "shortest_path"
      max_depth: 3
    malfunction_config:
      malfunction_rate: 0.001
      min_duration: 20
      max_duration: 50
    speed_ratios: 
      1.: 0.7
      0.5: 0.3
    reward_config: 0
    random_seed: 42